{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T15:30:38.204561Z",
     "iopub.status.busy": "2025-05-28T15:30:38.203891Z",
     "iopub.status.idle": "2025-05-28T15:33:43.381327Z",
     "shell.execute_reply": "2025-05-28T15:33:43.380252Z",
     "shell.execute_reply.started": "2025-05-28T15:30:38.204536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet unsloth vllm==0.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:34:33.863973Z",
     "iopub.status.busy": "2025-05-28T15:34:33.863686Z",
     "iopub.status.idle": "2025-05-28T15:35:07.830156Z",
     "shell.execute_reply": "2025-05-28T15:35:07.829509Z",
     "shell.execute_reply.started": "2025-05-28T15:34:33.863951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:38:26.195407Z",
     "iopub.status.busy": "2025-05-28T15:38:26.195103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_seq_length = 2048\n",
    "lora_rank = 64\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "# Adjust model with PEFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:34:24.287503Z",
     "iopub.status.busy": "2025-05-28T15:34:24.286823Z",
     "iopub.status.idle": "2025-05-28T15:34:24.355031Z",
     "shell.execute_reply": "2025-05-28T15:34:24.354113Z",
     "shell.execute_reply.started": "2025-05-28T15:34:24.287467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"5CD-AI/Vietnamese-meta-math-MetaMathQA-40K-gg-translated\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:23:08.146232Z",
     "iopub.status.busy": "2025-05-28T14:23:08.145523Z",
     "iopub.status.idle": "2025-05-28T14:23:11.941277Z",
     "shell.execute_reply": "2025-05-28T14:23:11.940520Z",
     "shell.execute_reply.started": "2025-05-28T14:23:08.146205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Regex pattern to extract answers from responses\n",
    "answer_pattern = re.compile(\n",
    "    r\"(đáp án là:|đáp án là :|câu trả lời là:|câu trả lời là :)\\s*(.*)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Process data and create formatted list\n",
    "formatted_dataset = []\n",
    "for item in dataset:\n",
    "    response = item[\"response_vi\"].strip().lower()\n",
    "    match = answer_pattern.search(response)\n",
    "    if match:\n",
    "        answer = match.group(2).strip()\n",
    "        formatted_dataset.append({\n",
    "            \"question\": item[\"query_vi\"],\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "# Define reasoning and solution tags\n",
    "reasoning_start = \"<thinking>\"\n",
    "reasoning_end = \"</thinking>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "# Create system prompt\n",
    "system_prompt = f\"\"\"\n",
    "You are given a problem.\n",
    "Think about the problem and provide your thought process.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your final answer between {solution_start} and {solution_end}.\n",
    "\"\"\"\n",
    "\n",
    "# Convert formatted list to dataset\n",
    "train_dataset = Dataset.from_list(formatted_dataset[:8000])\n",
    "train_dataset = train_dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": x[\"answer\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:23:18.734069Z",
     "iopub.status.busy": "2025-05-28T14:23:18.733381Z",
     "iopub.status.idle": "2025-05-28T14:23:18.739783Z",
     "shell.execute_reply": "2025-05-28T14:23:18.739214Z",
     "shell.execute_reply.started": "2025-05-28T14:23:18.734046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define regex to check response format\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}\\$\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"Kiểm tra chính xác định dạng của phản hồi.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"Kiểm tra định dạng một cách tương đối.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:23:22.422447Z",
     "iopub.status.busy": "2025-05-28T14:23:22.421731Z",
     "iopub.status.idle": "2025-05-28T14:23:22.430936Z",
     "shell.execute_reply": "2025-05-28T14:23:22.430147Z",
     "shell.execute_reply.started": "2025-05-28T14:23:22.422423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define regex to extract numbers from response\n",
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?([\\d\\.\\,]{1,})\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Kiểm tra phản hồi so với đáp án đúng.\"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            score -= 1.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Kiểm tra tính chính xác của các số được trích xuất.\"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    # Count number of checks\n",
    "    count = getattr(check_numbers, 'counter', 0) + 1\n",
    "    check_numbers.counter = count\n",
    "\n",
    "    # Display information every 5 checks\n",
    "    if count % 5 == 0:\n",
    "        print('*' * 20, f\"Question: {question}\",\n",
    "              f\"\\nResponse:\\n{responses[0]}\",\n",
    "              f\"\\nExtracted: {extracted_responses[0]}\",\n",
    "              f\"\\nGT Answer: {answer[0]}\")\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas in numbers\n",
    "            guess = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(1.5 if guess == true_answer else -0.5)\n",
    "        except ValueError:\n",
    "            scores.append(0)\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T14:31:36.011006Z",
     "iopub.status.busy": "2025-05-28T14:31:36.010296Z",
     "iopub.status.idle": "2025-05-28T14:31:39.337557Z",
     "shell.execute_reply": "2025-05-28T14:31:39.336834Z",
     "shell.execute_reply.started": "2025-05-28T14:31:36.010981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Determine maximum prompt length\n",
    "max_len = max(train_dataset.map(\n",
    "        lambda x: {\"tokens\": tokenizer.apply_chat_template(\n",
    "            x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
    "        batched=True,).map(lambda x: {\"length\": len(x[\"tokens\"])})[\"length\"])\n",
    "\n",
    "max_prompt_length = max_len + 1\n",
    "\n",
    "# Set training configuration\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=64,\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    save_steps=250,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"outputs_bz2\",\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T15:30:12.654Z",
     "iopub.execute_input": "2025-05-28T14:31:43.795442Z",
     "iopub.status.busy": "2025-05-28T14:31:43.794759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select first question index\n",
    "idx = 0\n",
    "\n",
    "# Create message list for model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": train_dataset[idx][\"question\"]},\n",
    "]\n",
    "\n",
    "# Set sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# Apply chat template to tokenizer\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Path to saved LoRA model\n",
    "path_lora = \"grpo_saved_lora\"\n",
    "\n",
    "# Generate response from model\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=model.load_lora(path_lora),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# Print results\n",
    "print(f\"Problem:\\n{train_dataset[idx]['question']}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(f\"GT Answer: {train_dataset[idx]['answer']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
